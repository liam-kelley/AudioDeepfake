[NeMo W 2023-03-23 02:40:48 optimizers:55] Apex was not found. Using the lamb or fused_adam optimizer will error out.
[NeMo W 2023-03-23 02:40:49 experimental:28] Module <class 'nemo.collections.common.tokenizers.text_to_speech.tts_tokenizers.IPATokenizer'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2023-03-23 02:40:49 experimental:28] Module <class 'nemo.collections.tts.models.radtts.RadTTSModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2023-03-23 02:40:49 nemo_logging:349] /home/liam/myvenv/lib/python3.7/site-packages/hydra/_internal/hydra.py:127: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
    See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
      configure_logging=with_log_configuration,
    
[NeMo I 2023-03-23 02:40:51 exp_manager:343] Experiments will be logged at exp/fastpitch_cruisetuningv2/FastPitch/2023-03-23_02-40-51
[NeMo I 2023-03-23 02:40:51 exp_manager:718] TensorboardLogger has been set up
[NeMo W 2023-03-23 02:40:51 exp_manager:989] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 600000. Please ensure that max_steps will run for at least 5 epochs to ensure that checkpointing will not error out.
[NeMo I 2023-03-23 02:40:54 tokenize_and_classify:87] Creating ClassifyFst grammars.
[NeMo W 2023-03-23 02:41:25 experimental:28] Module <class 'nemo_text_processing.g2p.modules.IPAG2P'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2023-03-23 02:41:26 modules:96] apply_to_oov_word=None, This means that some of words will remain unchanged if they are not handled by any of the rules in self.parse_one_word(). This may be intended if phonemes and chars are both valid inputs, otherwise, you may see unexpected deletions in your input.
[NeMo W 2023-03-23 02:41:27 nemo_logging:349] /home/liam/myvenv/lib/python3.7/site-packages/torch/jit/annotations.py:309: UserWarning: TorchScript will treat type annotations of Tensor dtype-specific subtypes as if they are normal Tensors. dtype constraints are not enforced in compilation either.
      warnings.warn("TorchScript will treat type annotations of Tensor "
    
[NeMo I 2023-03-23 02:41:27 data:217] Loading dataset from ./manifests/manifest_train_mult_speakers.json.
[NeMo I 2023-03-23 02:43:46 data:254] Loaded dataset with 5426 files.
[NeMo I 2023-03-23 02:43:46 data:256] Dataset contains 7.53 hours.
[NeMo I 2023-03-23 02:43:46 data:358] Pruned 0 files. Final dataset contains 5426 files
[NeMo I 2023-03-23 02:43:46 data:361] Pruned 0.00 hours. Final dataset contains 7.53 hours.
[NeMo I 2023-03-23 02:43:46 data:217] Loading dataset from ./manifests/manifest_val.json.
[NeMo I 2023-03-23 02:43:46 data:254] Loaded dataset with 8 files.
[NeMo I 2023-03-23 02:43:46 data:256] Dataset contains 0.01 hours.
[NeMo I 2023-03-23 02:43:46 data:358] Pruned 0 files. Final dataset contains 8 files
[NeMo I 2023-03-23 02:43:46 data:361] Pruned 0.00 hours. Final dataset contains 0.01 hours.
[NeMo I 2023-03-23 02:43:46 features:267] PADDING: 1
[NeMo I 2023-03-23 02:43:46 cloud:56] Found existing object /home/liam/.cache/torch/NeMo/NeMo_1.14.0/tts_en_fastpitch_align/b7d086a07b5126c12d5077d9a641a38c/tts_en_fastpitch_align.nemo.
[NeMo I 2023-03-23 02:43:46 cloud:62] Re-using file from: /home/liam/.cache/torch/NeMo/NeMo_1.14.0/tts_en_fastpitch_align/b7d086a07b5126c12d5077d9a641a38c/tts_en_fastpitch_align.nemo
[NeMo I 2023-03-23 02:43:46 common:912] Instantiating model from pre-trained checkpoint
[NeMo I 2023-03-23 02:43:50 tokenize_and_classify:87] Creating ClassifyFst grammars.
[NeMo W 2023-03-23 02:44:23 modules:96] apply_to_oov_word=None, This means that some of words will remain unchanged if they are not handled by any of the rules in self.parse_one_word(). This may be intended if phonemes and chars are both valid inputs, otherwise, you may see unexpected deletions in your input.
[NeMo W 2023-03-23 02:44:23 modelPT:143] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.
    Train config : 
    dataset:
      _target_: nemo.collections.tts.torch.data.TTSDataset
      manifest_filepath: /ws/LJSpeech/nvidia_ljspeech_train_clean_ngc.json
      sample_rate: 22050
      sup_data_path: /raid/LJSpeech/supplementary
      sup_data_types:
      - align_prior_matrix
      - pitch
      n_fft: 1024
      win_length: 1024
      hop_length: 256
      window: hann
      n_mels: 80
      lowfreq: 0
      highfreq: 8000
      max_duration: null
      min_duration: 0.1
      ignore_file: null
      trim: false
      pitch_fmin: 65.40639132514966
      pitch_fmax: 2093.004522404789
      pitch_norm: true
      pitch_mean: 212.35873413085938
      pitch_std: 68.52806091308594
      use_beta_binomial_interpolator: true
    dataloader_params:
      drop_last: false
      shuffle: true
      batch_size: 24
      num_workers: 0
    
[NeMo W 2023-03-23 02:44:23 modelPT:150] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). 
    Validation config : 
    dataset:
      _target_: nemo.collections.tts.torch.data.TTSDataset
      manifest_filepath: /ws/LJSpeech/nvidia_ljspeech_val_clean_ngc.json
      sample_rate: 22050
      sup_data_path: /raid/LJSpeech/supplementary
      sup_data_types:
      - align_prior_matrix
      - pitch
      n_fft: 1024
      win_length: 1024
      hop_length: 256
      window: hann
      n_mels: 80
      lowfreq: 0
      highfreq: 8000
      max_duration: null
      min_duration: null
      ignore_file: null
      trim: false
      pitch_fmin: 65.40639132514966
      pitch_fmax: 2093.004522404789
      pitch_norm: true
      pitch_mean: 212.35873413085938
      pitch_std: 68.52806091308594
      use_beta_binomial_interpolator: true
    dataloader_params:
      drop_last: false
      shuffle: false
      batch_size: 24
      num_workers: 0
    
[NeMo I 2023-03-23 02:44:23 features:267] PADDING: 1
[NeMo I 2023-03-23 02:44:24 save_restore_connector:243] Model FastPitchModel was successfully restored from /home/liam/.cache/torch/NeMo/NeMo_1.14.0/tts_en_fastpitch_align/b7d086a07b5126c12d5077d9a641a38c/tts_en_fastpitch_align.nemo.
[NeMo I 2023-03-23 02:44:24 modelPT:1115] Model checkpoint restored from pretrained checkpoint with name : `tts_en_fastpitch`
[NeMo I 2023-03-23 02:44:25 modelPT:602] Optimizer config = Adam (
    Parameter Group 0
        amsgrad: False
        betas: [0.9, 0.999]
        capturable: False
        differentiable: False
        eps: 1e-08
        foreach: None
        fused: False
        lr: 0.0002
        maximize: False
        weight_decay: 1e-06
    )
[NeMo I 2023-03-23 02:44:25 lr_scheduler:772] Scheduler not initialized as no `sched` config supplied to setup_optimizer()
[NeMo W 2023-03-23 09:44:36 nemo_logging:349] /home/liam/myvenv/lib/python3.7/site-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
      rank_zero_warn("Detected KeyboardInterrupt, attempting graceful shutdown...")
    
